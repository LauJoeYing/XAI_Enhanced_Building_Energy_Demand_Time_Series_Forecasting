{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_pipeline import transformation_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:/Downloads/final-year-project/data/preprocessed_train_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline object and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Downloads\\final-year-project\\notebooks\\data_fetcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(['building_id', 'meter', 'primary_use'],\n",
      "d:\\Downloads\\final-year-project\\notebooks\\data_fetcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"timestamp\"] = pd.to_datetime(df.loc[:, \"timestamp\"])\n",
      "d:\\Downloads\\final-year-project\\notebooks\\data_fetcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['season'] = df.month.apply(self.season_finder)\n",
      "d:\\Downloads\\final-year-project\\notebooks\\data_fetcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['weekend'] = df.timestamp.dt.dayofweek > 4\n",
      "d:\\Downloads\\final-year-project\\notebooks\\data_fetcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['day_of_the_week'] = df.timestamp.dt.dayofweek\n"
     ]
    }
   ],
   "source": [
    "pipeline, data_cleaned = transformation_pipeline(\n",
    "    data, building_id=122, meter=0, primary_use=99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data and showing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = pipeline.fit_transform(data_cleaned)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying the meter reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.36463482, 0.36902801, ..., 0.2800659 , 0.27594728,\n",
       "       0.27841845])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(transformed_data[:, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20923913, 0.38175676, 0.65927978, ..., 3.        , 0.        ,\n",
       "        4.        ],\n",
       "       [0.20652174, 0.38175676, 0.66897507, ..., 3.        , 0.        ,\n",
       "        4.        ],\n",
       "       [0.17663043, 0.36486486, 0.67313019, ..., 3.        , 0.        ,\n",
       "        4.        ],\n",
       "       ...,\n",
       "       [0.29347826, 0.49662162, 0.73268698, ..., 3.        , 1.        ,\n",
       "        5.        ],\n",
       "       [0.29347826, 0.51013514, 0.72160665, ..., 3.        , 1.        ,\n",
       "        5.        ],\n",
       "       [0.30978261, 0.52027027, 0.71191136, ..., 3.        , 1.        ,\n",
       "        5.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(transformed_data[:, 1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(transformed_data[:, 1:],\n",
    "                                                  transformed_data[:, 0],\n",
    "                                                  test_size=0.2,\n",
    "                                                  shuffle=False,\n",
    "                                                  random_state=2021)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating time series data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(x_train,\n",
    "                                                                y_train,\n",
    "                                                                length=6, sampling_rate=1,\n",
    "                                                                stride=1, batch_size=32\n",
    "                                                                )\n",
    "\n",
    "val_gen = tf.keras.preprocessing.sequence.TimeseriesGenerator(x_val,\n",
    "                                                              y_val,\n",
    "                                                              length=6, sampling_rate=1,\n",
    "                                                              stride=1, batch_size=32\n",
    "                                                              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.LSTM(128, activation='relu',\n",
    "                                                  return_sequences=False),\n",
    "                            tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.0001))\n",
    "\n",
    "cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                      patience=15,\n",
    "                                      restore_best_weights=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 3s 7ms/step - loss: 0.0644 - val_loss: 0.0217\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0381 - val_loss: 0.0154\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0233 - val_loss: 0.0143\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0159 - val_loss: 0.0149\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0249\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0362\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0090 - val_loss: 0.0438\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0084 - val_loss: 0.0559\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0084 - val_loss: 0.0647\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0092 - val_loss: 0.0739\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.0091 - val_loss: 0.0691\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0138 - val_loss: 0.0637\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0188 - val_loss: 0.0550\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0226 - val_loss: 0.0263\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0097 - val_loss: 0.0166\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0065 - val_loss: 0.0104\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0062 - val_loss: 0.0082\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0060 - val_loss: 0.0072\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0058 - val_loss: 0.0076\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0057 - val_loss: 0.0083\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0087\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0087\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0085\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0052 - val_loss: 0.0081\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0075\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0068\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0047 - val_loss: 0.0064\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0047 - val_loss: 0.0090\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0118\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0048 - val_loss: 0.0150\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0144\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0048 - val_loss: 0.0183\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0050 - val_loss: 0.0153\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.0051 - val_loss: 0.0293\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0055 - val_loss: 0.0200\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0063 - val_loss: 0.0290\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0056 - val_loss: 0.0154\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0106 - val_loss: 0.0188\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0094 - val_loss: 0.0080\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0078 - val_loss: 0.0119\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0043 - val_loss: 0.0069\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    epochs=100,\n",
    "                    callbacks=[cb],\n",
    "                    shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance (Before Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 3ms/step\n",
      "MSE: 0.006446781877679553\n",
      "RMSE: 0.08029185436692537\n",
      "MAE: 0.06263138654604979\n",
      "MAPE: 18.32984341213807\n",
      "SMAPE: 20.40293297596518\n",
      "Forecast Bias: -0.015752323125649056\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Helper function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Helper function to calculate SMAPE\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Assuming you have the generators `train_gen` and `val_gen`\n",
    "y_true_val = np.concatenate([y for x, y in val_gen], axis=0)\n",
    "y_pred_val = model.predict(val_gen).flatten()\n",
    "\n",
    "mse = mean_squared_error(y_true_val, y_pred_val)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true_val, y_pred_val)\n",
    "mape = mean_absolute_percentage_error(y_true_val, y_pred_val)\n",
    "smape = symmetric_mean_absolute_percentage_error(y_true_val, y_pred_val)\n",
    "forecast_bias = np.mean(y_pred_val - y_true_val)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}')\n",
    "print(f'SMAPE: {smape}')\n",
    "print(f'Forecast Bias: {forecast_bias}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reason why the tuner not being implemented here is because the tuner do not have learning rate tuning option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested: Units=50, Dropout=0.1, LR=0.001, Optimizer=adam, Val Loss=0.014581742696464062\n",
      "Tested: Units=50, Dropout=0.1, LR=0.001, Optimizer=rmsprop, Val Loss=0.006966582499444485\n",
      "Tested: Units=50, Dropout=0.1, LR=0.001, Optimizer=sgd, Val Loss=0.030222374945878983\n",
      "Tested: Units=50, Dropout=0.1, LR=0.01, Optimizer=adam, Val Loss=0.005528555251657963\n",
      "Tested: Units=50, Dropout=0.1, LR=0.01, Optimizer=rmsprop, Val Loss=0.004463735967874527\n",
      "Tested: Units=50, Dropout=0.1, LR=0.01, Optimizer=sgd, Val Loss=nan\n",
      "Tested: Units=50, Dropout=0.2, LR=0.001, Optimizer=adam, Val Loss=0.010578668676316738\n",
      "Tested: Units=50, Dropout=0.2, LR=0.001, Optimizer=rmsprop, Val Loss=0.007940234616398811\n",
      "Tested: Units=50, Dropout=0.2, LR=0.001, Optimizer=sgd, Val Loss=0.010313737206161022\n",
      "Tested: Units=50, Dropout=0.2, LR=0.01, Optimizer=adam, Val Loss=0.0071882507763803005\n",
      "Tested: Units=50, Dropout=0.2, LR=0.01, Optimizer=rmsprop, Val Loss=0.005468044430017471\n",
      "Tested: Units=50, Dropout=0.2, LR=0.01, Optimizer=sgd, Val Loss=0.021801413968205452\n",
      "Tested: Units=50, Dropout=0.3, LR=0.001, Optimizer=adam, Val Loss=0.026515355333685875\n",
      "Tested: Units=50, Dropout=0.3, LR=0.001, Optimizer=rmsprop, Val Loss=0.012608829885721207\n",
      "Tested: Units=50, Dropout=0.3, LR=0.001, Optimizer=sgd, Val Loss=0.030986890196800232\n",
      "Tested: Units=50, Dropout=0.3, LR=0.01, Optimizer=adam, Val Loss=0.012892622500658035\n",
      "Tested: Units=50, Dropout=0.3, LR=0.01, Optimizer=rmsprop, Val Loss=0.02569420449435711\n",
      "Tested: Units=50, Dropout=0.3, LR=0.01, Optimizer=sgd, Val Loss=nan\n",
      "Tested: Units=100, Dropout=0.1, LR=0.001, Optimizer=adam, Val Loss=0.006929248105734587\n",
      "Tested: Units=100, Dropout=0.1, LR=0.001, Optimizer=rmsprop, Val Loss=0.02902250550687313\n",
      "Tested: Units=100, Dropout=0.1, LR=0.001, Optimizer=sgd, Val Loss=0.0197750274091959\n",
      "Tested: Units=100, Dropout=0.1, LR=0.01, Optimizer=adam, Val Loss=0.004019944928586483\n",
      "Tested: Units=100, Dropout=0.1, LR=0.01, Optimizer=rmsprop, Val Loss=0.004250294528901577\n",
      "Tested: Units=100, Dropout=0.1, LR=0.01, Optimizer=sgd, Val Loss=nan\n",
      "Tested: Units=100, Dropout=0.2, LR=0.001, Optimizer=adam, Val Loss=0.0056375968270003796\n",
      "Tested: Units=100, Dropout=0.2, LR=0.001, Optimizer=rmsprop, Val Loss=0.006223278120160103\n",
      "Tested: Units=100, Dropout=0.2, LR=0.001, Optimizer=sgd, Val Loss=0.02376842498779297\n",
      "Tested: Units=100, Dropout=0.2, LR=0.01, Optimizer=adam, Val Loss=0.009117773734033108\n",
      "Tested: Units=100, Dropout=0.2, LR=0.01, Optimizer=rmsprop, Val Loss=0.00665327487513423\n",
      "Tested: Units=100, Dropout=0.2, LR=0.01, Optimizer=sgd, Val Loss=nan\n",
      "Tested: Units=100, Dropout=0.3, LR=0.001, Optimizer=adam, Val Loss=0.014235811308026314\n",
      "Tested: Units=100, Dropout=0.3, LR=0.001, Optimizer=rmsprop, Val Loss=0.015988977625966072\n",
      "Tested: Units=100, Dropout=0.3, LR=0.001, Optimizer=sgd, Val Loss=0.039669834077358246\n",
      "Tested: Units=100, Dropout=0.3, LR=0.01, Optimizer=adam, Val Loss=0.005288264714181423\n",
      "Tested: Units=100, Dropout=0.3, LR=0.01, Optimizer=rmsprop, Val Loss=0.008164958097040653\n",
      "Tested: Units=100, Dropout=0.3, LR=0.01, Optimizer=sgd, Val Loss=nan\n",
      "Tested: Units=150, Dropout=0.1, LR=0.001, Optimizer=adam, Val Loss=0.00895147118717432\n",
      "Tested: Units=150, Dropout=0.1, LR=0.001, Optimizer=rmsprop, Val Loss=0.02241434156894684\n",
      "Tested: Units=150, Dropout=0.1, LR=0.001, Optimizer=sgd, Val Loss=0.02079717256128788\n",
      "Tested: Units=150, Dropout=0.1, LR=0.01, Optimizer=adam, Val Loss=0.01588033325970173\n",
      "Tested: Units=150, Dropout=0.1, LR=0.01, Optimizer=rmsprop, Val Loss=0.005538176279515028\n",
      "Tested: Units=150, Dropout=0.1, LR=0.01, Optimizer=sgd, Val Loss=nan\n",
      "Tested: Units=150, Dropout=0.2, LR=0.001, Optimizer=adam, Val Loss=0.01613014005124569\n",
      "Tested: Units=150, Dropout=0.2, LR=0.001, Optimizer=rmsprop, Val Loss=0.022071026265621185\n",
      "Tested: Units=150, Dropout=0.2, LR=0.001, Optimizer=sgd, Val Loss=0.019614171236753464\n",
      "Tested: Units=150, Dropout=0.2, LR=0.01, Optimizer=adam, Val Loss=0.013345994986593723\n",
      "Tested: Units=150, Dropout=0.2, LR=0.01, Optimizer=rmsprop, Val Loss=0.008285058662295341\n",
      "Tested: Units=150, Dropout=0.2, LR=0.01, Optimizer=sgd, Val Loss=nan\n",
      "Tested: Units=150, Dropout=0.3, LR=0.001, Optimizer=adam, Val Loss=0.015610940754413605\n",
      "Tested: Units=150, Dropout=0.3, LR=0.001, Optimizer=rmsprop, Val Loss=0.014379468746483326\n",
      "Tested: Units=150, Dropout=0.3, LR=0.001, Optimizer=sgd, Val Loss=0.029978031292557716\n",
      "Tested: Units=150, Dropout=0.3, LR=0.01, Optimizer=adam, Val Loss=0.03143911808729172\n",
      "Tested: Units=150, Dropout=0.3, LR=0.01, Optimizer=rmsprop, Val Loss=0.049651797860860825\n",
      "Tested: Units=150, Dropout=0.3, LR=0.01, Optimizer=sgd, Val Loss=nan\n",
      "Best setting found:\n",
      "Units: 100, Dropout: 0.1, Learning Rate: 0.01, Optimizer: adam, Loss: 0.004019944928586483\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Function to create and compile the LSTM model\n",
    "def build_and_compile_lstm_model(input_shape, lstm_units, dropout, learning_rate, optimizer_choice):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.LSTM(lstm_units, activation='relu', input_shape=input_shape, dropout=dropout),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Configure the optimizer based on the selection\n",
    "    optimizers_dict = {\n",
    "        'adam': optimizers.Adam(learning_rate=learning_rate),\n",
    "        'rmsprop': optimizers.RMSprop(learning_rate=learning_rate),\n",
    "        'sgd': optimizers.SGD(learning_rate=learning_rate)\n",
    "    }\n",
    "    optimizer = optimizers_dict[optimizer_choice]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# List of hyperparameters to try\n",
    "lstm_units_options = [50, 100, 150]\n",
    "dropout_options = [0.1, 0.2, 0.3]\n",
    "learning_rate_options = [0.001, 0.01]\n",
    "optimizer_options = ['adam', 'rmsprop', 'sgd']\n",
    "\n",
    "# Input shape of the data\n",
    "input_shape = (6, 12)  # 6 timesteps and 12 features per timestep\n",
    "\n",
    "# Track the best setting\n",
    "best_setting = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for units in lstm_units_options:\n",
    "    for dropout in dropout_options:\n",
    "        for lr in learning_rate_options:\n",
    "            for optimizer in optimizer_options:\n",
    "                # Build and compile the model with current settings\n",
    "                model = build_and_compile_lstm_model(input_shape, units, dropout, lr, optimizer)\n",
    "\n",
    "                # Fit the model (assuming train_gen and val_gen are defined)\n",
    "                history = model.fit(train_gen, validation_data=val_gen, epochs=10, verbose=0)\n",
    "\n",
    "                # Evaluate the model\n",
    "                val_loss = model.evaluate(val_gen, verbose=0)\n",
    "\n",
    "                print(f\"Tested: Units={units}, Dropout={dropout}, LR={lr}, Optimizer={optimizer}, Val Loss={val_loss}\")\n",
    "\n",
    "                # Update the best model if current model is better\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_setting = (units, dropout, lr, optimizer)\n",
    "\n",
    "# Print the best setting after all iterations\n",
    "print(\"Best setting found:\")\n",
    "print(f\"Units: {best_setting[0]}, Dropout: {best_setting[1]}, Learning Rate: {best_setting[2]}, Optimizer: {best_setting[3]}, Loss: {best_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 2s 4ms/step - loss: 0.1812 - val_loss: 0.0109\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0141 - val_loss: 0.0083\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0104 - val_loss: 0.0065\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0095 - val_loss: 0.0054\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0087 - val_loss: 0.0064\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0086 - val_loss: 0.0074\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0080 - val_loss: 0.0061\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0078 - val_loss: 0.0060\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0069 - val_loss: 0.0094\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0074 - val_loss: 0.0144\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0067 - val_loss: 0.0125\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0061 - val_loss: 0.0103\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0085 - val_loss: 0.0165\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0070 - val_loss: 0.0061\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0079 - val_loss: 0.0083\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0087 - val_loss: 0.0072\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.0071 - val_loss: 0.0115\n",
      "{'loss': [0.18121355772018433, 0.014113133773207664, 0.011289678514003754, 0.010356828570365906, 0.009518418461084366, 0.008681116625666618, 0.008557320572435856, 0.008047429844737053, 0.00784890167415142, 0.007928771898150444, 0.006938562728464603, 0.007373185828328133, 0.006666358094662428, 0.006084566004574299, 0.00641758693382144, 0.008540262468159199, 0.007008028216660023, 0.00787997618317604, 0.008654243312776089, 0.007085783407092094], 'val_loss': [0.010942026972770691, 0.008270113728940487, 0.009150777012109756, 0.006489036604762077, 0.0053915041498839855, 0.006425568368285894, 0.007379163522273302, 0.006084808614104986, 0.006021565292030573, 0.0072425976395606995, 0.009355670772492886, 0.014369610697031021, 0.012466737069189548, 0.010332096368074417, 0.0079659977927804, 0.016515977680683136, 0.006091342307627201, 0.008283529430627823, 0.007221928332000971, 0.0114777572453022]}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the model\n",
    "final_model = tf.keras.Sequential([\n",
    "    # Adding an LSTM layer with 100 units and applying dropout within the LSTM\n",
    "    tf.keras.layers.LSTM(100, activation='relu', return_sequences=False, dropout=0.1),\n",
    "    # Output layer with a single unit (for regression tasks)\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model with the specified optimizer and learning rate\n",
    "final_model.compile(\n",
    "    loss='mse',  # Mean Squared Error as the loss function\n",
    "    optimizer=tf.keras.optimizers.Adam(0.01)  # Adam optimizer with a learning rate of 0.01\n",
    ")\n",
    "\n",
    "# Callback for early stopping to prevent overfitting\n",
    "cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Assuming 'train_gen' and 'val_gen' are predefined data generators for training and validation\n",
    "# Fit the model using the training generator and validate using the validation generator\n",
    "history = final_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=100,\n",
    "    callbacks=[cb],\n",
    "    shuffle=False  # Important for time series data to maintain sequence integrity\n",
    ")\n",
    "\n",
    "# Optional: Print out the history to see how the loss and validation loss progressed\n",
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 1ms/step\n",
      "MSE: 0.005391505183610941\n",
      "RMSE: 0.07342686962965901\n",
      "MAE: 0.056744420113253605\n",
      "MAPE: 14.7194370963044\n",
      "SMAPE: 14.907820619282484\n",
      "Forecast Bias: -0.015065061250218096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Helper function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Helper function to calculate SMAPE\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Assuming you have the generators `train_gen` and `val_gen`\n",
    "y_true_val = np.concatenate([y for x, y in val_gen], axis=0)\n",
    "y_pred_val = final_model.predict(val_gen).flatten()\n",
    "\n",
    "mse = mean_squared_error(y_true_val, y_pred_val)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true_val, y_pred_val)\n",
    "mape = mean_absolute_percentage_error(y_true_val, y_pred_val)\n",
    "smape = symmetric_mean_absolute_percentage_error(y_true_val, y_pred_val)\n",
    "forecast_bias = np.mean(y_pred_val - y_true_val)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}')\n",
    "print(f'SMAPE: {smape}')\n",
    "print(f'Forecast Bias: {forecast_bias}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM_ADAM\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/LSTM_ADAM\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_model.save('models/LSTM_ADAM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
